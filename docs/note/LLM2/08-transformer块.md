# Transformer 块

## 记忆

1. 自注意力层；前馈神经网络；
2. 自注意力层：整合上下文信息；当前词汇关注与其他词的相关性；
3. 多头注意力，视角；合并视角，生成高维向量；
4. 自注意力，看其他；前馈神经网络，处理自己；
5. 自注意力-模拟；前馈神经网络-推理；

## 理解

1. 自注意力层：当前词汇关注与其他词的相关性；
2. 自注意力-模拟；前馈神经网络-推理；
